import gradio as gr 
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from collections import Counter
import torch
from nltk.corpus import wordnet
import nltk
from langdetect import detect, LangDetectException  # Import langdetect

nltk.download('wordnet')

# Load BART tokenizer and model for summarization
tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn")

# Check if GPU is available and use it if possible
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Function to summarize key points
def extract_main_points(text, num_points=5):
    summary = summarize_bart(text, max_length=150, min_length=80)
    sentences = summary.split('. ')
    points = [f"• {sentence.strip()}." for sentence in sentences[:num_points] if sentence.strip()]
    return "\n".join(points)

# Function to extract key nouns and adjectives from the text
def extract_key_terms(text, num_concepts=10):
    words = nltk.word_tokenize(text)
    pos_tags = nltk.pos_tag(words)
    relevant_words = [word for word, pos in pos_tags if pos in ["NN", "JJ"] and len(word) > 3]
    most_common_words = [word for word, _ in Counter(relevant_words).most_common(num_concepts)]
    return most_common_words

# Function to generate a list of concepts with their definitions
def extract_concepts_with_definitions(text, num_concepts=10):
    try:
        # Detect the language of the input text
        language = detect(text)
    except LangDetectException:
        return "Could not detect the language of the text."
    
    # If the language is not English, return a message
    if language != 'en':
        return "The 'Concepts List' feature is only supported in English."

    key_terms = extract_key_terms(text, num_concepts)
    definitions = []
    
    for term in set(key_terms):
        # Try to get the definition from WordNet
        synsets = wordnet.synsets(term)
        if synsets:
            definition = synsets[0].definition()
            definitions.append(f"• {term} = {definition.strip()}")
        else:
            # No dictionary definition found, skip adding this term
            continue

    # If no definitions were found, return a message
    if not definitions:
        return "No definitions found."

    return "\n".join(definitions)

# Function to summarize the text using the BART model
def summarize_bart(input_text, max_length, min_length):
    inputs = tokenizer(input_text, return_tensors="pt", max_length=1024, truncation=True)
    inputs = inputs.to(device)
    summary_ids = model.generate(inputs["input_ids"], max_length=max_length, min_length=min_length, do_sample=False)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Function to generate a summary based on the selected summary type
def generate_summary(input_text, url, file, format_type, source):
    if source == "Text Input":
        content = input_text
    elif source == "Web Page URL":
        content = url
    elif source == "File Upload" and file is not None:
        content = file.read().decode('utf-8')
    else:
        content = ""

    if not content:
        return "No content to summarize."

    try:
        if format_type == "Main Points":
            summary = extract_main_points(content)
        elif format_type == "Concepts List":
            summary = extract_concepts_with_definitions(content)
        elif format_type == "Short Summary":
            summary = summarize_bart(content, max_length=80, min_length=40)
        elif format_type == "Medium Summary":
            summary = summarize_bart(content, max_length=200, min_length=100)
        elif format_type == "Long Summary":
            summary = summarize_bart(content, max_length=400, min_length=200)
        else:
            summary = content
    except Exception as e:
        return f"Error during summarization: {str(e)}"

    return summary

# Function to dynamically control input visibility based on source type
def dynamic_input(source):
    if source == "Text Input":
        return gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)
    elif source == "Web Page URL":
        return gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)
    elif source == "File Upload":
        return gr.update(visible=False), gr.update(visible=False), gr.update(visible=True)
    return gr.update(visible=False), gr.update(visible=False), gr.update(visible(False))

# Build Gradio interface
with gr.Blocks() as demo:
    gr.Markdown("### Summary App")

    # Input type selection
    source = gr.Dropdown(["Text Input", "Web Page URL", "File Upload"], label="Input type", value="Text Input", interactive=True)

    # Input fields
    text_input_box = gr.Textbox(label="Enter Text", visible=True)
    file_input_box = gr.File(label="Upload a File", visible=False)
    url_input_box = gr.Textbox(label="Give Website URL", visible=False)

    # Summary type selection
    format_type = gr.Dropdown(
        ["Main Points", "Concepts List", "Short Summary", "Medium Summary", "Long Summary"],
        label="Choose Summary Type",
        value="Main Points"
    )

    # Output box for the generated summary
    summary_output_box = gr.Textbox(label="Generate Summary", visible=True, lines=10)

    # Button to trigger summary generation
    summary_button = gr.Button("Generate Summary")

    # Trigger the summary generation
    summary_button.click(
        fn=generate_summary,
        inputs=[text_input_box, url_input_box, file_input_box, format_type, source],
        outputs=[summary_output_box]
    )

    # Dynamically update input visibility based on source type
    source.change(fn=dynamic_input, inputs=source, outputs=[text_input_box, file_input_box, url_input_box])

# Launch the interface
demo.launch()
